#установка библиотек и импорт модулей - 1-ая йчейка если в Google Colab, в локальной IDE (PyCharm, VSCode) - установить в терминале без ! 

!pip install --upgrade great_expectations 

import os
import shutil
import random
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import great_expectations as gx
import warnings

from faker import Faker

############################ 2-ая йчейка если в Google Colab #########################################
#Генерация parquet-файлов
N_BASE = 100000            # базовое количество записей
DUP_RATE = 0.12           # доля дублированных строк (точные дубли)
OUT_DIR = "/content/customers_2025"
NULL_PROBS = {
    "email": 0.07,
    "phone": 0.12,
    "address": 0.08,
    "city": 0.06,
    "region": 0.06,
    "postal_code": 0.05,
    "birth_date": 0.03,
    "income": 0.07,
    "last_purchase_date": 0.25,  # у части клиентов нет покупок
}

# Инициализация
fake = Faker("ru_RU")

# Вспомогательные функции
def random_dates_in_2025(n: int) -> pd.Series:
    start = pd.Timestamp("2025-01-01")
    end = pd.Timestamp("2025-12-31")
    days = (end - start).days + 1
    offs = np.random.randint(0, days, size=n)
    return start + pd.to_timedelta(offs, unit="D")

def random_date_between(start_ts: pd.Timestamp, end_ts: pd.Timestamp) -> pd.Timestamp:
    if pd.isna(start_ts) or pd.isna(end_ts) or end_ts < start_ts:
        return pd.NaT
    delta_days = (end_ts - start_ts).days
    if delta_days < 0:
        return pd.NaT
    return start_ts + pd.to_timedelta(np.random.randint(0, delta_days + 1), unit="D")

def clip_int(arr, min_v):
    arr = np.array(arr, dtype=float)
    arr = np.where(arr < min_v, min_v, arr)
    return arr.astype(int)

# Генерация базовых данных
record_date = random_dates_in_2025(N_BASE)
first_names = [fake.first_name() for _ in range(N_BASE)]
last_names  = [fake.last_name() for _ in range(N_BASE)]

birth_dates = [pd.Timestamp(fake.date_of_birth(minimum_age=18, maximum_age=80)) for _ in range(N_BASE)]

emails = [fake.free_email() for _ in range(N_BASE)]
phones = [fake.phone_number() for _ in range(N_BASE)]
cities = [fake.city() for _ in range(N_BASE)]
regions = [fake.region() for _ in range(N_BASE)]
postal_codes = [fake.postcode() for _ in range(N_BASE)]
countries = ["Россия"] * N_BASE
addresses = [fake.address().replace("\n", ", ") for _ in range(N_BASE)]

# signup_date не позже record_date
signup_dates = []
for i in range(N_BASE):
    rd = record_date[i]
    # Регистрация с 2015-01-01 до даты записи
    start = pd.Timestamp("2015-01-01")
    signup_dates.append(random_date_between(start, rd))


# последняя покупка между signup_date и record_date для части клиентов
last_purchase_dates = []
for i in range(N_BASE):
    if np.random.rand() < (1 - NULL_PROBS["last_purchase_date"]):
        last_purchase_dates.append(random_date_between(signup_dates[i], record_date[i]))
    else:
        last_purchase_dates.append(pd.NaT)

# сумма трат зависима от наличия покупок
total_spent = []
for lp in last_purchase_dates:
    if pd.isna(lp):
        total_spent.append(0.0 if np.random.rand() < 0.9 else float(np.random.gamma(1.5, 20000)))
    else:
        total_spent.append(float(np.random.gamma(2.0, 35000)))
total_spent = np.round(total_spent, 2)

# VIP как функция трат и случайности
is_vip = (np.array(total_spent) > 300000) | (np.random.rand(N_BASE) < 0.03)

# churned, если давно не покупал относительно даты записи
churned = []
for i in range(N_BASE):
    rd = record_date[i]
    lp = last_purchase_dates[i]
    if pd.isna(lp):
        churned.append(bool(np.random.rand() < 0.6))
    else:
        churned.append(bool((rd - lp).days > 180))

df_base = pd.DataFrame({
    "record_date": record_date,
    "first_name": first_names,
    "last_name": last_names,
    "birth_date": birth_dates,
    "email": emails,
    "phone": phones,
    "address": addresses,
    "city": cities,
    "region": regions,
    "postal_code": postal_codes,
    "country": countries,
    "signup_date": signup_dates,
    "last_purchase_date": last_purchase_dates,
    "total_spent": total_spent,
    "is_vip": is_vip,
    "churned": churned,
})

# Добавляем дубли
n_dups = int(N_BASE * DUP_RATE)
dup_idx = np.random.choice(df_base.index, size=n_dups, replace=True)
df_dups = df_base.loc[dup_idx].copy()
df = pd.concat([df_base, df_dups], ignore_index=True)

# Вставляем пропуски
for col, p in NULL_PROBS.items():
    if col not in df.columns:
        continue
    mask = np.random.rand(len(df)) < p
    if np.issubdtype(df[col].dtype, np.number):
        df.loc[mask, col] = np.nan
    else:
        df.loc[mask, col] = None

# Партиционирование по месяцам и сохранение в Parquet
# Готовим метки месяца для имени файла
df["mm"] = df["record_date"].dt.strftime("%m")
df["yy"] = df["record_date"].dt.strftime("%y")
df["month"] = df["record_date"].dt.strftime("%Y-%m")  # оставлю для отчёта

# Чистим выходную папку, если была
if os.path.exists(OUT_DIR):
    shutil.rmtree(OUT_DIR)
os.makedirs(OUT_DIR, exist_ok=True)

# Сохраняем по одному файлу на месяц в OUT_DIR
for (mm, yy), part in df.groupby(["mm", "yy"]):
    fname = f"cust_{mm}_{yy}.parquet"  # например, cust_01_25.parquet
    part_out = part.drop(columns=["mm", "yy"])  # служебные колонки не пишем
    part_out.to_parquet(
        os.path.join(OUT_DIR, fname),
        engine="pyarrow",
        index=False,
        compression="snappy",
    )

# Вывод в консоли
print(f"Датасет сохранён в: {OUT_DIR}")
print(f"Базовых строк: {len(df_base):,}".replace(",", " "))
print(f"Добавлено дублей: {len(df_dups):,}".replace(",", " "))
print(f"Итого строк: {len(df):,}".replace(",", " "))
print("\nРаспределение по месяцам:")
print(df["month"].value_counts().sort_index())


############################ 3-ая йчейка если в Google Colab #######################################
#Проверка качества данных
context = gx.get_context() #создание контекста для Great Epectations
warnings.filterwarnings("ignore", category=DeprecationWarning) #отключить вывод предупреждений

# Определяем источник данных по имени и создаём его при необходимости
ds_name = "my_pandas_data_source"

if ds_name in context.data_sources.all():
    data_source = context.data_sources.get(ds_name)
else:
    data_source = context.data_sources.add_pandas(name=ds_name)


# Имя актива данных внутри источника
asset_name = "my_clients_data"

# Получаем существующий актив или создаём новый
if asset_name in data_source.get_asset_names():
    data_asset = data_source.get_asset(asset_name)
else:
    data_asset = data_source.add_dataframe_asset(name=asset_name)



# Имя батча (набор данных внутри актива)
batch_name = "full_clients_batch"

# Попытка получить существующее определение батча, иначе создаём батч по всему DataFrame
try:
    batch_definition = data_asset.get_batch_definition(batch_name)
except Exception:
    batch_definition = data_asset.add_batch_definition_whole_dataframe(name=batch_name)

# Формируем батч из переданного DataFrame
batch = batch_definition.get_batch({"dataframe": df})


# Имя набора проверок (suite)
suite_name = "customer_data_quality_suite"

# Попытка использовать существующую секцию, иначе создать новую
try:
    suite = context.suites.get(suite_name)
    print(f"Using existing suite: {suite_name}")
except Exception:
    suite = context.suites.add(gx.ExpectationSuite(name=suite_name))


# Добавляем в suite ожидание: уникальность значений по столбцам email и phone
suite.add_expectation(gx.expectations.ExpectCompoundColumnsToBeUnique(column_list=["email", "phone"]))


# Проверка пропусков поля last_purchase_date, их подсчет и вывод
if "last_purchase_date" in df.columns:
    missing_last_purchase = df["last_purchase_date"].isna().sum()
    total_rows = len(df)
    missing_pct = (missing_last_purchase / total_rows) * 100 if total_rows > 0 else 0.0
    print(f"Доля пропусков в столбце last_purchase_date (время последней покупки): {int(missing_last_purchase)} из {total_rows} ({missing_pct:.2f}%)")
    # Добавляем в suite время последней покупки: столбец не должен содержать пропуски
    suite.add_expectation(gx.expectations.ExpectColumnValuesToNotBeNull(column="last_purchase_date"))
else:
    print("Столбец last_purchase_date отсутствует в данных, пропуски не подсчитаны")


# Выполняем валидацию батча против набора проверок
validation_result = batch.validate(suite)

print(validation_result)
print(f"\nПроверка качества завершена, качество датасета: {validation_result.success}")

# Вывод количества дублей по столбцам email и phone
total_rows = len(df)
duplicate_count_all = df.duplicated(subset=["email", "phone"], keep=False).sum()

duplicate_fraction = (duplicate_count_all / total_rows) if total_rows > 0 else 0.0
print(f"Найдено {int(duplicate_count_all)} дублей по полям email и phone")
print(f"Доля дублей от общего числа строк: {duplicate_fraction:.2%} ({int(duplicate_count_all)} из {total_rows})")
